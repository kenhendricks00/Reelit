import whisper
import os

# Global variable to hold the loaded model
_model = None

def load_whisper_model(model_name="tiny.en"):
    """Loads the specified Whisper model. Defaults to tiny English model."""
    global _model
    if _model is None:
        print(f"Loading Whisper model: {model_name}...")
        try:
            _model = whisper.load_model(model_name)
            print("Whisper model loaded successfully.")
        except Exception as e:
            print(f"Error loading Whisper model '{model_name}': {e}")
            print("Please ensure torch and ffmpeg are installed correctly.")
            print("Try running: pip install -U openai-whisper")
            print("And ensure ffmpeg is in your PATH.")
            raise # Re-raise the exception to stop the process
    return _model

def get_word_timestamps(audio_path, model_name="tiny.en"):
    """Transcribes audio using Whisper and returns word-level timestamps.

    Args:
        audio_path (str): Path to the audio file (e.g., MP3).
        model_name (str): Name of the Whisper model to use (e.g., tiny.en, base.en).

    Returns:
        list: A list of dictionaries, where each dictionary contains
              'word', 'start', and 'end' keys for each word.
              Returns None if transcription fails.
    """
    if not os.path.exists(audio_path):
        print(f"Error: Audio file not found at {audio_path}")
        return None

    try:
        model = load_whisper_model(model_name)
    except Exception:
        return None # Model loading failed

    print(f"Transcribing {audio_path} with Whisper for word timestamps...")
    try:
        # Set word_timestamps=True
        result = model.transcribe(audio_path, word_timestamps=True, fp16=False) # fp16=False might be more stable on CPU
        
        word_segments = []
        if 'segments' in result:
            for segment in result['segments']:
                if 'words' in segment:
                     # Adjust word dict to match expected format if necessary
                     # Whisper format is often [{'word': ' Hello', 'start': 0.5, 'end': 0.8, 'probability': 0.99}, ...]
                     for word_info in segment['words']:
                         # Clean up leading/trailing whitespace from whisper word
                         clean_word = word_info['word'].strip()
                         if clean_word: # Only add if there's actual word content
                            word_segments.append({
                                'word': clean_word,
                                'start': word_info['start'],
                                'end': word_info['end']
                            })
        
        if not word_segments:
            print("Warning: Whisper transcription did not return any word segments.")
            return None
            
        print(f"Transcription complete. Found {len(word_segments)} word timestamps.")
        return word_segments

    except Exception as e:
        print(f"Error during Whisper transcription: {e}")
        return None

if __name__ == '__main__':
    # Example usage:
    # Assumes you have a test audio file (e.g., the one generated by tts_generator)
    test_audio = "../output/test_narration_timed.mp3" # Adjust if needed

    if not os.path.exists(test_audio):
        print(f"Test audio file not found: {test_audio}")
        print("Please generate a test audio file first (e.g., by running video_creator.py example once).")
    else:
        print("Running Whisper timestamp extraction example...")
        timestamps = get_word_timestamps(test_audio)

        if timestamps:
            print("\n--- Example Word Timestamps ---")
            for i, word_info in enumerate(timestamps[:15]): # Print first 15 words
                print(f"  {word_info['word']} ({word_info['start']:.2f}s - {word_info['end']:.2f}s)")
            print("...")
        else:
            print("Failed to get word timestamps from the audio.") 